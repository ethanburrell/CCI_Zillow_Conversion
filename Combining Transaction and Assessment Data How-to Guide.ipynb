{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 60 ms, total: 456 ms\n",
      "Wall time: 631 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sqlalchemy\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import pymysql.cursors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a guide to help convert the Zillow Transaction data to individual CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Get the data from the Zillow Files\n",
    "\n",
    "* We need to get the Main table from the Assessor Data, and then the Main and PropertyInfo table. \n",
    "    * We can convert the Main.txt files for both Assessment and Transaction files in \"Trans and Asmt Data to Database.ipynb\" (USE THIS FOR PROPERTY INFO AS WELL)\n",
    "    * ~~The propert info can be done with \"Property Info Data to Database.ipynb\"~~\n",
    "    \n",
    "Running this will result in a lot of different .db files. These are sqlite3 db files, you can interact with them by using SQL and running 'sqlite3' in terminal in the directory where your files are located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Join the databases\n",
    "\n",
    "To link the Assessor data to Transaction data, we need to do a lot of joins. \n",
    "\n",
    "From the Zillow Documentation:\n",
    "\n",
    "> To match events to property attribute information in assessor tables, first join the Main table to PropertyInfo using TransID. Use the ImportParcelID in the PropertyInfo table to link to the property in assessor tables. Use the ImportParcelID to match previous sales or other events on the same property. (TableDescriptions_2016-01.xsls)\n",
    "\n",
    "So we need to join Transaction to Property info using \"TransId\", then that resulting table to the Assessor Data using the \"ImportParcelID\" collumn. We have to do this in terminal because we do not have enough RAM to do this in pandas.\n",
    "\n",
    "\n",
    "In Terminal (Example is for CT). Navigate to correct folder using \"cd\" and then type sqlite3 hit enter, it will open up a prompt:\n",
    "```\n",
    "attach 'CT_trans.db' as trans;\n",
    "attach 'CT_property.db' as prop;\n",
    "CREATE TABLE prop.joined AS SELECT * from trans.fips a inner join prop.PropertyInfo b on a.TransId = b.TransId;\n",
    "```\n",
    "This leaves you with a table called \"joined\" in 'CT_property.db', with the Transaction PropertyInfo and Main joined.\n",
    "\n",
    "Where \"CT_trans.db\" is the transaction database for CT and \"CT_property.db\" is the property database for CT.\n",
    "Please check the tables in the second line, to make sure that each database has the correct table that you are trying to join. Also, I would highly recommend filtering by FIPS before this step.\n",
    "\n",
    "Then to join to Asmt:\n",
    "```\n",
    "attach 'CT_asmt.db' as asmt;\n",
    "attach 'CT_property.db' as prop;\n",
    "CREATE TABLE asmt.combined AS SELECT * from asmt.fips a inner join prop.joined b on a.ImportParcelID = b.ImportParcelID;\n",
    "```\n",
    "Now you have a table called \"combined\" in \"CT_asmt.db\", this has the Transaction and Assesment Data joined!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Break into different CSVs for each Unique FIPS\n",
    "\n",
    "[Look at this tutorial for how to export a CSV from sqlite3 terminal](http://www.sqlitetutorial.net/sqlite-export-csv/)\n",
    "\n",
    "Here we are going to do something very annoying, we are going to create the individual CSVs by hand. I tried to write scripts for this (and spent a great deal of time doing so) but was unable to do it :(\n",
    "\n",
    "Once you have the joined DBs, we can split these up into individual DBs for each fips and then convert each db into a CSV. I know this is a pain.\n",
    "\n",
    "Use the \"Joined DB to FIPS CSV\" notebook to generate all of this code, then you just have to copy and paste it in to terminal and wait for it to finish.\n",
    "\n",
    "To make each indivudial Database copy and paste the \"sqlite3 <state><fips>.db\" in terminal, then type .database, then .quit. This will create each individual database.\n",
    "    \n",
    "Then we want to run one instance of \n",
    "\n",
    "\n",
    "\n",
    "**Q**: I got \"Error: too many attached databases - max 10\" \n",
    "\n",
    "**A**: L close the sqlite3 instance and relaunch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
